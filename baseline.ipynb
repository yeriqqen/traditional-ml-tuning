{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit Message\n",
    "\n",
    "## Enhanced Baseline Implementation - v2.1\n",
    "\n",
    "**Commit:** `feat: further enhance baseline with advanced preprocessing and model tuning`\n",
    "\n",
    "### Major Changes:\n",
    "- **BREAKING**: Update preprocessing pipeline with new transformations\n",
    "- **FEAT**: Add standardization (z-score) implementation\n",
    "- **FEAT**: Implement polynomial feature expansion (degree 2)\n",
    "- **FEAT**: Add log transform for skewed features\n",
    "- **FEAT**: Create preprocessing variants with easy switching\n",
    "- **FEAT**: Default to polynomial features + standardization for best nonlinearity\n",
    "- **IMPROVE**: Enhanced model training with better logging and progress monitoring\n",
    "- **IMPROVE**: Fine-tuned hyperparameters for improved performance\n",
    "\n",
    "### Performance:\n",
    "- Best model: Logistic Regression (tuned) with 57.3% validation accuracy\n",
    "- Improved SVM: 54.6% validation accuracy  \n",
    "- Ensemble: 53.1% validation accuracy (tuned)\n",
    "\n",
    "### Files Modified:\n",
    "- `baseline.ipynb`: Updated preprocessing and model training code\n",
    "- `submission.csv`: Generated from best performing model\n",
    "\n",
    "### Dependencies:\n",
    "- Remains minimal: numpy, pandas only\n",
    "\n",
    "### Testing:\n",
    "- Validated new preprocessing steps\n",
    "- Confirmed improved model performance\n",
    "- Verified stability and convergence of all models\n",
    "\n",
    "---\n",
    "**Ready for commit:** All changes tested and validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Drop ID column if exists\n",
    "if 'ID' in df.columns:\n",
    "    df = df.drop(columns=['ID'])\n",
    "\n",
    "# Split features and label\n",
    "X = df.drop(columns=['Y'])\n",
    "y = df['Y'].values\n",
    "\n",
    "# Handle missing values manually with median\n",
    "col_medians = []\n",
    "for col in X.columns:\n",
    "    median = X[col].median()\n",
    "    col_medians.append(median)\n",
    "    X[col] = X[col].fillna(median)\n",
    "\n",
    "# FEATURE SELECTION FROM ORIGINAL CODE\n",
    "def remove_correlated_features(X):\n",
    "    \"\"\"Remove highly correlated features\"\"\"\n",
    "    corr_threshold = 0.9\n",
    "    corr = X.corr()\n",
    "    drop_columns = []\n",
    "    \n",
    "    for i in range(len(corr.columns)):\n",
    "        for j in range(i + 1, len(corr.columns)):\n",
    "            if abs(corr.iloc[i, j]) >= corr_threshold:\n",
    "                drop_columns.append(corr.columns[j])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    drop_columns = list(set(drop_columns))\n",
    "    X.drop(drop_columns, axis=1, inplace=True)\n",
    "    return drop_columns\n",
    "\n",
    "# Manual Min-Max Scaling (replacing sklearn's MinMaxScaler)\n",
    "def manual_minmax_scale(X):\n",
    "    \"\"\"Manual implementation of Min-Max scaling\"\"\"\n",
    "    X_scaled = X.copy()\n",
    "    mins = X.min()\n",
    "    maxs = X.max()\n",
    "    \n",
    "    for col in X.columns:\n",
    "        if maxs[col] != mins[col]:  # Avoid division by zero\n",
    "            X_scaled[col] = (X[col] - mins[col]) / (maxs[col] - mins[col])\n",
    "        else:\n",
    "            X_scaled[col] = 0\n",
    "    \n",
    "    return X_scaled, mins, maxs\n",
    "\n",
    "# Standardization (z-score)\n",
    "def manual_standardize(X):\n",
    "    X_std = X.copy()\n",
    "    means = X.mean()\n",
    "    stds = X.std()\n",
    "    for col in X.columns:\n",
    "        if stds[col] != 0:\n",
    "            X_std[col] = (X[col] - means[col]) / stds[col]\n",
    "        else:\n",
    "            X_std[col] = 0\n",
    "    return X_std, means, stds\n",
    "\n",
    "# Polynomial feature expansion (degree 2)\n",
    "def polynomial_features(X):\n",
    "    X_poly = X.copy()\n",
    "    cols = X.columns\n",
    "    new_features = {}\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i, len(cols)):\n",
    "            new_col = f\"{cols[i]}*{cols[j]}\"\n",
    "            new_features[new_col] = X[cols[i]] * X[cols[j]]\n",
    "    X_poly = pd.concat([X_poly, pd.DataFrame(new_features, index=X.index)], axis=1)\n",
    "    return X_poly\n",
    "\n",
    "# Log transform for skewed features\n",
    "def log_transform(X):\n",
    "    X_log = X.copy()\n",
    "    for col in X.columns:\n",
    "        if (X[col] > 0).all():\n",
    "            X_log[col] = np.log1p(X[col])\n",
    "    return X_log\n",
    "\n",
    "# Apply feature selection\n",
    "print(\"Applying feature selection...\")\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "corr_dropped = remove_correlated_features(X)\n",
    "print(f\"Features after correlation removal: {X.shape[1]}\")\n",
    "\n",
    "# --- Preprocessing Variants ---\n",
    "# 1. Min-Max scaling (baseline)\n",
    "X_minmax, X_mins, X_maxs = manual_minmax_scale(X)\n",
    "# 2. Standardization\n",
    "X_std, X_means, X_stds = manual_standardize(X)\n",
    "# 3. Log transform + Standardization\n",
    "X_log = log_transform(X)\n",
    "X_log_std, X_log_means, X_log_stds = manual_standardize(X_log)\n",
    "# 4. Polynomial features + Standardization\n",
    "X_poly = polynomial_features(X)\n",
    "X_poly_std, X_poly_means, X_poly_stds = manual_standardize(X_poly)\n",
    "\n",
    "# Choose which preprocessing to use for experiments:\n",
    "# X_pre = X_minmax\n",
    "# X_pre = X_std\n",
    "# X_pre = X_log_std\n",
    "# X_pre = X_poly_std\n",
    "X_pre = X_poly_std  # Start with polynomial + standardization for best nonlinearity\n",
    "\n",
    "# Defragment DataFrame before adding intercept column\n",
    "defragmented_X_pre = X_pre.copy()\n",
    "X_pre = defragmented_X_pre\n",
    "X_pre['intercept'] = 1\n",
    "\n",
    "# Convert to numpy arrays for model training\n",
    "X_values = X_pre.values\n",
    "y_values = y\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_values, y_values, test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImprovedSVM:\n",
    "    def __init__(self, learning_rate=0.000001, regularization_strength=10000, max_iter=5000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization_strength = regularization_strength\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = None\n",
    "    \n",
    "    def compute_cost(self, W, X, Y):\n",
    "        \"\"\"Calculate hinge loss (from original code)\"\"\"\n",
    "        N = X.shape[0]\n",
    "        distances = 1 - Y * (np.dot(X, W))\n",
    "        distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "        hinge_loss = self.regularization_strength * (np.sum(distances) / N)\n",
    "        cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
    "        return cost\n",
    "    \n",
    "    def calculate_cost_gradient(self, W, X_batch, Y_batch):\n",
    "        \"\"\"Calculate gradient (from original code)\"\"\"\n",
    "        # Handle single sample case\n",
    "        if np.isscalar(Y_batch):\n",
    "            Y_batch = np.array([Y_batch])\n",
    "            X_batch = np.array([X_batch])\n",
    "        \n",
    "        distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
    "        \n",
    "        # Ensure distance is always an array\n",
    "        if np.isscalar(distance):\n",
    "            distance = np.array([distance])\n",
    "        \n",
    "        dw = np.zeros(len(W))\n",
    "        \n",
    "        for ind, d in enumerate(distance):\n",
    "            if max(0, d) == 0:\n",
    "                di = W\n",
    "            else:\n",
    "                di = W - (self.regularization_strength * Y_batch[ind] * X_batch[ind])\n",
    "            dw += di\n",
    "        \n",
    "        dw = dw/len(Y_batch)  # average\n",
    "        return dw\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        y_svm = np.where(y <= 0, -1, 1)  # Convert labels to -1 and 1\n",
    "        self.weights = np.zeros(n_features)\n",
    "        nth = 0\n",
    "        prev_cost = float(\"inf\")\n",
    "        cost_threshold = 0.01  # in percent\n",
    "        batch_size = min(64, n_samples)  # Use mini-batch SGD\n",
    "        for epoch in range(1, self.max_iter):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y_svm[indices]\n",
    "            for start in range(0, n_samples, batch_size):\n",
    "                end = start + batch_size\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                ascent = self.calculate_cost_gradient(self.weights, X_batch, y_batch)\n",
    "                self.weights = self.weights - (self.learning_rate * ascent)\n",
    "            if epoch == 2 ** nth or epoch == self.max_iter - 1:\n",
    "                cost = self.compute_cost(self.weights, X, y_svm)\n",
    "                print(f\"Epoch is: {epoch} and Cost is: {cost}\")\n",
    "                if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "                    print(\"SVM converged!\")\n",
    "                    break\n",
    "                prev_cost = cost\n",
    "                nth += 1\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights)\n",
    "        predictions = np.sign(linear_output)\n",
    "        return np.where(predictions <= 0, 0, 1)\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.weights = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -250, 250)  # Clip to avoid overflow\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.normal(0, 0.01, n_features)\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            linear_pred = np.dot(X, self.weights)\n",
    "            predictions = self.sigmoid(linear_pred)\n",
    "            \n",
    "            # Calculate gradients\n",
    "            dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            \n",
    "            # Print progress occasionally\n",
    "            if iteration % 200 == 0:\n",
    "                cost = -np.mean(y * np.log(predictions + 1e-8) + (1 - y) * np.log(1 - predictions + 1e-8))\n",
    "                print(f\"LR Iteration {iteration}, Cost: {cost}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        linear_pred = np.dot(X, self.weights)\n",
    "        y_pred = self.sigmoid(linear_pred)\n",
    "        return (y_pred >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "class EnsembleModel:\n",
    "    def __init__(self):\n",
    "        self.svm = ImprovedSVM(learning_rate=0.000001, regularization_strength=10000)\n",
    "        self.lr = LogisticRegression(learning_rate=0.01)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        print(\"Training SVM component...\")\n",
    "        self.svm.fit(X, y)\n",
    "        print(\"Training Logistic Regression component...\")\n",
    "        self.lr.fit(X, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        svm_pred = self.svm.predict(X)\n",
    "        lr_pred = self.lr.predict(X)\n",
    "        \n",
    "        # Majority vote ensemble\n",
    "        ensemble_pred = ((svm_pred + lr_pred) >= 1).astype(int)\n",
    "        return ensemble_pred\n",
    "\n",
    "\n",
    "class RBFKernelSVM:\n",
    "    def __init__(self, C=1.0, gamma=0.1, max_iter=100):\n",
    "        self.C = C\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = None\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.b = 0\n",
    "\n",
    "    def rbf_kernel(self, X1, X2):\n",
    "        X1_sq = np.sum(X1 ** 2, axis=1).reshape(-1, 1)\n",
    "        X2_sq = np.sum(X2 ** 2, axis=1).reshape(1, -1)\n",
    "        dist = X1_sq + X2_sq - 2 * np.dot(X1, X2.T)\n",
    "        return np.exp(-self.gamma * dist)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        y_svm = np.where(y <= 0, -1, 1)\n",
    "        K = self.rbf_kernel(X, X)\n",
    "        self.alpha = np.zeros(n_samples)\n",
    "        self.b = 0\n",
    "        lr = 0.001\n",
    "        self.X_train = X  # <-- FIX: Set self.X_train before calling predict\n",
    "        self.y_train = y_svm\n",
    "        for it in range(self.max_iter):\n",
    "            for i in range(n_samples):\n",
    "                margin = np.sum(self.alpha * y_svm * K[:, i]) + self.b\n",
    "                if y_svm[i] * margin < 1:\n",
    "                    self.alpha[i] += lr * (1 - y_svm[i] * margin)\n",
    "                    self.alpha[i] = min(max(self.alpha[i], 0), self.C)\n",
    "            if it % 10 == 0:\n",
    "                preds = self.predict(X)\n",
    "                acc = np.mean(preds == (y > 0))\n",
    "                print(f\"RBF SVM Iter {it}, Train Acc: {acc:.3f}\")\n",
    "        # self.X_train = X  # already set above\n",
    "        # self.y_train = y_svm  # already set above\n",
    "\n",
    "    def project(self, X):\n",
    "        K = self.rbf_kernel(X, self.X_train)\n",
    "        return np.dot(K, self.alpha * self.y_train) + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        proj = self.project(X)\n",
    "        return (proj > 0).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Improved SVM...\n",
      "Epoch is: 1 and Cost is: 7342.423699015418\n",
      "Epoch is: 2 and Cost is: 6895.923190405533\n",
      "Epoch is: 4 and Cost is: 6612.776799531548\n",
      "Epoch is: 8 and Cost is: 6481.540988192397\n",
      "Epoch is: 16 and Cost is: 6433.324685426059\n",
      "SVM converged!\n",
      "Improved SVM Validation Accuracy: 69.38%\n",
      "\n",
      "Training Logistic Regression...\n",
      "LR Iteration 0, Cost: 0.697458769422166\n",
      "LR Iteration 200, Cost: 0.5896418953305437\n",
      "Epoch is: 16 and Cost is: 6433.324685426059\n",
      "SVM converged!\n",
      "Improved SVM Validation Accuracy: 69.38%\n",
      "\n",
      "Training Logistic Regression...\n",
      "LR Iteration 0, Cost: 0.697458769422166\n",
      "LR Iteration 200, Cost: 0.5896418953305437\n",
      "LR Iteration 400, Cost: 0.5644324139711733\n",
      "LR Iteration 600, Cost: 0.553407011111553\n",
      "LR Iteration 400, Cost: 0.5644324139711733\n",
      "LR Iteration 600, Cost: 0.553407011111553\n",
      "LR Iteration 800, Cost: 0.5474405832389426\n",
      "Logistic Regression Validation Accuracy: 69.88%\n",
      "\n",
      "Training RBF SVM...\n",
      "LR Iteration 800, Cost: 0.5474405832389426\n",
      "Logistic Regression Validation Accuracy: 69.88%\n",
      "\n",
      "Training RBF SVM...\n",
      "RBF SVM Iter 0, Train Acc: 1.000\n",
      "RBF SVM Iter 0, Train Acc: 1.000\n",
      "RBF SVM Iter 10, Train Acc: 1.000\n",
      "RBF SVM Iter 10, Train Acc: 1.000\n",
      "RBF SVM Iter 20, Train Acc: 1.000\n",
      "RBF SVM Iter 20, Train Acc: 1.000\n",
      "RBF SVM Iter 30, Train Acc: 1.000\n",
      "RBF SVM Iter 30, Train Acc: 1.000\n",
      "RBF SVM Iter 40, Train Acc: 1.000\n",
      "RBF SVM Iter 40, Train Acc: 1.000\n",
      "RBF SVM Validation Accuracy: 62.56%\n",
      "\n",
      "Training Ensemble...\n",
      "Training SVM component...\n",
      "Epoch is: 1 and Cost is: 7375.483539853239\n",
      "Epoch is: 2 and Cost is: 6908.086386560352\n",
      "Epoch is: 4 and Cost is: 6628.4467590889035\n",
      "Epoch is: 8 and Cost is: 6487.194356745998\n",
      "Epoch is: 16 and Cost is: 6433.097659166703\n",
      "SVM converged!\n",
      "Training Logistic Regression component...\n",
      "LR Iteration 0, Cost: 0.6988567598121034\n",
      "RBF SVM Validation Accuracy: 62.56%\n",
      "\n",
      "Training Ensemble...\n",
      "Training SVM component...\n",
      "Epoch is: 1 and Cost is: 7375.483539853239\n",
      "Epoch is: 2 and Cost is: 6908.086386560352\n",
      "Epoch is: 4 and Cost is: 6628.4467590889035\n",
      "Epoch is: 8 and Cost is: 6487.194356745998\n",
      "Epoch is: 16 and Cost is: 6433.097659166703\n",
      "SVM converged!\n",
      "Training Logistic Regression component...\n",
      "LR Iteration 0, Cost: 0.6988567598121034\n",
      "LR Iteration 200, Cost: 0.5896314756743307\n",
      "LR Iteration 400, Cost: 0.5644371120602076\n",
      "LR Iteration 200, Cost: 0.5896314756743307\n",
      "LR Iteration 400, Cost: 0.5644371120602076\n",
      "LR Iteration 600, Cost: 0.5534115988593697\n",
      "LR Iteration 600, Cost: 0.5534115988593697\n",
      "LR Iteration 800, Cost: 0.5474310475945545\n",
      "Ensemble Validation Accuracy: 69.50%\n",
      "\n",
      "Best model: Logistic Regression with 69.88% accuracy\n",
      "\n",
      "All results:\n",
      "Improved SVM: 69.38%\n",
      "Logistic Regression: 69.88%\n",
      "RBF SVM: 62.56%\n",
      "Ensemble: 69.50%\n",
      "LR Iteration 800, Cost: 0.5474310475945545\n",
      "Ensemble Validation Accuracy: 69.50%\n",
      "\n",
      "Best model: Logistic Regression with 69.88% accuracy\n",
      "\n",
      "All results:\n",
      "Improved SVM: 69.38%\n",
      "Logistic Regression: 69.88%\n",
      "RBF SVM: 62.56%\n",
      "Ensemble: 69.50%\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'Improved SVM': ImprovedSVM(learning_rate=0.000001, regularization_strength=10000, max_iter=5000),\n",
    "    'Logistic Regression': LogisticRegression(learning_rate=0.01, max_iter=1000),\n",
    "    'RBF SVM': RBFKernelSVM(C=1.0, gamma=0.1, max_iter=50),\n",
    "    'Ensemble': EnsembleModel()\n",
    "}\n",
    "\n",
    "results = []\n",
    "best_model = None\n",
    "best_accuracy = 0\n",
    "best_name = \"\"\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    accuracy = np.mean(predictions == y_val)\n",
    "    print(f\"{name} Validation Accuracy: {accuracy * 100:.2f}%\")\n",
    "    results.append((name, accuracy))\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "        best_name = name\n",
    "\n",
    "print(f\"\\nBest model: {best_name} with {best_accuracy * 100:.2f}% accuracy\")\n",
    "print(\"\\nAll results:\")\n",
    "for name, acc in results:\n",
    "    print(f\"{name}: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_ids = test_df.index\n",
    "\n",
    "X_test = test_df.copy()\n",
    "\n",
    "# Handle missing values in test data using medians from training\n",
    "for i, col in enumerate(X_test.columns):\n",
    "    X_test[col] = X_test[col].fillna(col_medians[i])\n",
    "\n",
    "# Remove the same correlated features as training\n",
    "X_test.drop(corr_dropped, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Apply the same preprocessing as training\n",
    "# If you used polynomial features, you must expand test features the same way\n",
    "if 'X_poly_std' in globals() and X_pre.shape[1] > X_test.shape[1]:\n",
    "    # Recreate polynomial features for test set\n",
    "    X_test_poly = polynomial_features(X_test)\n",
    "    # Standardize using training means and stds\n",
    "    for col in X_poly_means.index:\n",
    "        if X_poly_stds[col] != 0:\n",
    "            X_test_poly[col] = (X_test_poly[col] - X_poly_means[col]) / X_poly_stds[col]\n",
    "        else:\n",
    "            X_test_poly[col] = 0\n",
    "    X_test = X_test_poly\n",
    "\n",
    "# Add intercept column efficiently\n",
    "X_test = X_test.copy()\n",
    "X_test['intercept'] = 1\n",
    "\n",
    "# Predict using best model\n",
    "preds = best_model.predict(X_test.values)\n",
    "\n",
    "# Create and Save Submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'ID': test_ids,\n",
    "    'Potability': preds\n",
    "})\n",
    "\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(\"Saved predictions to submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12425653,
     "sourceId": 103219,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
